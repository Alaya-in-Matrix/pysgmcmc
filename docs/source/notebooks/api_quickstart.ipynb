{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "import pysgmcmc as pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiating a Sampler\n",
    "\n",
    "To instantiate a sampler, we need two ingredients:\n",
    "* Target parameters of the sampler: a list of `tensorflow.Variable` objects \n",
    "* A cost function: callable that maps these target parameters to a 1-d `tensorflow.Tensor` representing their corresponding costs\n",
    "\n",
    "Note: In MCMC literature, the target parameters are often denoted as $\\theta$ and the cost function is frequently referred to as $U(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target parameters\n",
    "parameters = [tf.Variable(0.), tf.Variable(0.)]\n",
    "\n",
    "# cost function\n",
    "# XXX: Use some meaningful cost function here\n",
    "def cost_fun(params):\n",
    "    raise NotImplementedError(\"...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these ingredients, we can instantiate any of our samplers within a `tensorflow.Session`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0a9b03aabccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m sampler = SGHMCSampler(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/pysgmcmc/pysgmcmc/samplers/sghmc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, cost_fun, seed, batch_generator, epsilon, session, burn_in_steps, scale_grad, dtype, mdecay)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m#  }}} Initialize graph constants #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         grads = [vectorize(gradient) for gradient in\n",
      "\u001b[0;32m<ipython-input-28-0834a4a9d45b>\u001b[0m in \u001b[0;36mcost_fun\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# XXX: Use some meaningful cost function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcost_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ..."
     ]
    }
   ],
   "source": [
    "from pysgmcmc.samplers.sghmc import SGHMCSampler\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "sampler = SGHMCSampler(\n",
    "    params=parameters, cost_fun=cost_fun, session=session, dtype=tf.float32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using data minibatches\n",
    "\n",
    "TODO: Explain data_batches.py and how to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available samplers\n",
    "\n",
    "TODO: INSERT HYPERLINK TO DOKU BELOW\n",
    "\n",
    "\n",
    "To get an overview of which samplers are available for use, examine our [documentation](http://pysgmcmc.readthedocs.io/en/latest/) or simply run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pysgmcmc.samplers in pysgmcmc:\n",
      "\n",
      "NAME\n",
      "    pysgmcmc.samplers\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    relativistic_hmc\n",
      "    relativistic_hmc2\n",
      "    relativistic_sghmc\n",
      "    sghmc\n",
      "    sgld\n",
      "    svgd\n",
      "\n",
      "CLASSES\n",
      "    pysgmcmc.sampling.BurnInMCMCSampler(pysgmcmc.sampling.MCMCSampler)\n",
      "        pysgmcmc.samplers.sghmc.SGHMCSampler\n",
      "        pysgmcmc.samplers.sgld.SGLDSampler\n",
      "    \n",
      "    class SGHMCSampler(pysgmcmc.sampling.BurnInMCMCSampler)\n",
      "     |  Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses a burn-in\n",
      "     |  procedure to adapt its own hyperparameters during the initial stages\n",
      "     |  of sampling.\n",
      "     |  \n",
      "     |  See [1] for more details on this burn-in procedure.\n",
      "     |  See [2] for more details on Stochastic Gradient Hamiltonian Monte-Carlo.\n",
      "     |  \n",
      "     |  [1] J. T. Springenberg, A. Klein, S. Falkner, F. Hutter\n",
      "     |      Bayesian Optimization with Robust Bayesian Neural Networks.\n",
      "     |      In Advances in Neural Information Processing Systems 29 (2016).\n",
      "     |  \n",
      "     |  [2] T. Chen, E. B. Fox, C. Guestrin\n",
      "     |      Stochastic Gradient Hamiltonian Monte Carlo\n",
      "     |      In Proceedings of Machine Learning Research 32 (2014).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGHMCSampler\n",
      "     |      pysgmcmc.sampling.BurnInMCMCSampler\n",
      "     |      pysgmcmc.sampling.MCMCSampler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, params, cost_fun, seed=None, batch_generator=None, epsilon=0.01, session=None, burn_in_steps=3000, scale_grad=1.0, dtype=tf.float64, mdecay=0.05)\n",
      "     |      Initialize the sampler parameters and set up a tensorflow.Graph\n",
      "     |          for later queries.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : list of tensorflow.Variable objects\n",
      "     |          Target parameters for which we want to sample new values.\n",
      "     |      \n",
      "     |      cost_fun : callable\n",
      "     |          Function that takes `params` as input and returns a\n",
      "     |          1-d `tensorflow.Tensor` that contains the cost-value.\n",
      "     |          Frequently denoted with `U` in literature.\n",
      "     |      \n",
      "     |      seed : int, optional\n",
      "     |          Random seed to use.\n",
      "     |          Defaults to `None`.\n",
      "     |      \n",
      "     |      batch_generator : iterable, optional\n",
      "     |          Iterable which returns dictionaries to feed into\n",
      "     |          tensorflow.Session.run() calls to evaluate the cost function.\n",
      "     |          Defaults to `None` which indicates that no batches shall be fed.\n",
      "     |      \n",
      "     |      epsilon : float, optional\n",
      "     |          Value that is used as learning rate parameter for the sampler,\n",
      "     |          also denoted as discretization parameter in literature.\n",
      "     |          Defaults to `0.01`.\n",
      "     |      \n",
      "     |      session : tensorflow.Session, optional\n",
      "     |          Session object which knows about the external part of the graph\n",
      "     |          (which defines `Cost`, and possibly batches).\n",
      "     |          Used internally to evaluate (burn-in/sample) the sampler.\n",
      "     |      \n",
      "     |      burn_in_steps: int, optional\n",
      "     |          Number of burn-in steps to perform. In each burn-in step, this\n",
      "     |          sampler will adapt its own internal parameters to decrease its error.\n",
      "     |          For reference see: TODO ADD PAPER REFERENCE HERE\n",
      "     |      \n",
      "     |      scale_grad : float, optional\n",
      "     |          Value that is used to scale the magnitude of the noise used\n",
      "     |          during sampling. In a typical batches-of-data setting this usually\n",
      "     |          corresponds to the number of examples in the entire dataset.\n",
      "     |          Defaults to `1.0` which corresponds to no scaling.\n",
      "     |      \n",
      "     |      dtype : tensorflow.DType, optional\n",
      "     |          Type of elements of `tensorflow.Tensor` objects used in this sampler.\n",
      "     |          Defaults to `tensorflow.float64`.\n",
      "     |      \n",
      "     |      mdecay : float, optional\n",
      "     |          (Constant) momentum decay per time-step.\n",
      "     |          Defaults to `0.05`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ----------\n",
      "     |      Simple, plain example:\n",
      "     |      TODO: Add 2D Gaussian Case here\n",
      "     |      \n",
      "     |      Simple example that uses batches:\n",
      "     |      TODO: Add simplified batch example here\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      ----------\n",
      "     |      pysgmcmc.sampling.BurnInMCMCSampler:\n",
      "     |          Base class for `SGHMCSampler` that specifies how actual sampling\n",
      "     |          is performed (using iterator protocol, e.g. `next(sampler)`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      "     |  \n",
      "     |  __next__(self)\n",
      "     |      Perform a sampler step:\n",
      "     |          Compute and return the next sample and next cost values\n",
      "     |          for this sampler.\n",
      "     |      \n",
      "     |          While `self.is_burning_in` returns `True`\n",
      "     |          (while the sampler has not yet performed `self.burn_in_steps`\n",
      "     |          steps) this will also adapt the samplers mass matrix in a\n",
      "     |          sampler-specific way to improve performance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sample: list of numpy.ndarray objects\n",
      "     |          Sampled values are a `numpy.ndarray` for each target parameter.\n",
      "     |      \n",
      "     |      cost: numpy.ndarray (1,)\n",
      "     |          Current cost value of the last evaluated target parameter values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      "     |  \n",
      "     |  is_burning_in\n",
      "     |      Check if this sampler is still in burn-in phase.\n",
      "     |          Used during graph construction to insert conditionals into the\n",
      "     |          graph that will make the sampler skip all burn-in operations\n",
      "     |          after the burn-in phase is over.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_burning_in: boolean\n",
      "     |          `True` if `self.n_iterations <= self.burn_in_steps`, otherwise `False`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      "     |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      "     |      \n",
      "     |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      "     |      directly, and then acts as a mix-in class.  You can also register\n",
      "     |      unrelated concrete classes (even built-in classes) and unrelated\n",
      "     |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      "     |      be considered subclasses of the registering ABC by the built-in\n",
      "     |      issubclass() function, but the registering ABC won't show up in\n",
      "     |      their MRO (Method Resolution Order) nor will method\n",
      "     |      implementations defined by the registering ABC be callable (not\n",
      "     |      even via super()).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pysgmcmc.sampling.MCMCSampler:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Allows using samplers as iterators.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ----------\n",
      "     |      Extract the first three thousand samples (with costs) from a sampler:\n",
      "     |      \n",
      "     |      >>> import tensorflow as tf\n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from itertools import islice\n",
      "     |      >>> from pysgmcmc.samplers.sghmc import SGHMCSampler\n",
      "     |      >>> session = tf.Session()\n",
      "     |      >>> x = tf.Variable(1.0)\n",
      "     |      >>> dist = tf.contrib.distributions.Normal(loc=0., scale=1.)\n",
      "     |      >>> n_burn_in, n_samples = 1000, 2000\n",
      "     |      >>> sampler = SGHMCSampler(params=[x], burn_in_steps=n_burn_in, cost_fun=lambda x: -dist.log_prob(x), session=session, dtype=tf.float32)\n",
      "     |      >>> session.run(tf.global_variables_initializer())\n",
      "     |      >>> burn_in_samples = list(islice(sampler, n_burn_in))  # perform all burn_in steps\n",
      "     |      >>> samples = list(islice(sampler, n_samples))\n",
      "     |      >>> len(burn_in_samples), len(samples)\n",
      "     |      (1000, 2000)\n",
      "     |      >>> session.close()\n",
      "     |      >>> tf.reset_default_graph()  # to avoid polluting test environment\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pysgmcmc.sampling.MCMCSampler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SGLDSampler(pysgmcmc.sampling.BurnInMCMCSampler)\n",
      "     |  Stochastic Gradient Langevin Dynamics Sampler that uses a burn-in\n",
      "     |  procedure to adapt its own hyperparameters during the initial stages\n",
      "     |  of sampling.\n",
      "     |  \n",
      "     |  See [1] for more details on this burn-in procedure.\n",
      "     |  See [2] for more details on Stochastic Gradient Langevin Dynamics.\n",
      "     |  \n",
      "     |  [1] J. T. Springenberg, A. Klein, S. Falkner, F. Hutter\n",
      "     |      Bayesian Optimization with Robust Bayesian Neural Networks.\n",
      "     |      In Advances in Neural Information Processing Systems 29 (2016).\n",
      "     |  \n",
      "     |  [2] M.Welling, Y. W. Teh\n",
      "     |      Bayesian Learning via Stochastic Gradient Langevin Dynamics\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGLDSampler\n",
      "     |      pysgmcmc.sampling.BurnInMCMCSampler\n",
      "     |      pysgmcmc.sampling.MCMCSampler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, params, cost_fun, seed=None, batch_generator=None, epsilon=0.01, session=None, burn_in_steps=3000, scale_grad=1.0, dtype=tf.float64, A=1.0)\n",
      "     |      Initialize the sampler parameters and set up a tensorflow.Graph\n",
      "     |          for later queries.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : list of tensorflow.Variable objects\n",
      "     |          Target parameters for which we want to sample new values.\n",
      "     |      \n",
      "     |      cost_fun : callable\n",
      "     |          Function that takes `params` as input and returns a\n",
      "     |          1-d `tensorflow.Tensor` that contains the cost-value.\n",
      "     |          Frequently denoted with `U` in literature.\n",
      "     |      \n",
      "     |      seed : int, optional\n",
      "     |          Random seed to use.\n",
      "     |          Defaults to `None`.\n",
      "     |      \n",
      "     |      batch_generator : BatchGenerator, optional\n",
      "     |          Iterable which returns dictionaries to feed into\n",
      "     |          tensorflow.Session.run() calls to evaluate the cost function.\n",
      "     |          Defaults to `None` which indicates that no batches shall be fed.\n",
      "     |      \n",
      "     |      epsilon : float, optional\n",
      "     |          Value that is used as learning rate parameter for the sampler,\n",
      "     |          also denoted as discretization parameter in literature.\n",
      "     |          Defaults to `0.01`.\n",
      "     |      \n",
      "     |      session : tensorflow.Session, optional\n",
      "     |          Session object which knows about the external part of the graph\n",
      "     |          (which defines `Cost`, and possibly batches).\n",
      "     |          Used internally to evaluate (burn-in/sample) the sampler.\n",
      "     |      \n",
      "     |      burn_in_steps: int, optional\n",
      "     |          Number of burn-in steps to perform. In each burn-in step, this\n",
      "     |          sampler will adapt its own internal parameters to decrease its error.\n",
      "     |          For reference see: TODO ADD PAPER REFERENCE HERE\n",
      "     |      \n",
      "     |      scale_grad : float, optional\n",
      "     |          Value that is used to scale the magnitude of the noise used\n",
      "     |          during sampling. In a typical batches-of-data setting this usually\n",
      "     |          corresponds to the number of examples in the entire dataset.\n",
      "     |      \n",
      "     |      A : float, optional\n",
      "     |          TODO XXX Doku\n",
      "     |          Defaults to `1.0`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ----------\n",
      "     |      Simple, plain example:\n",
      "     |      TODO: Add more samples\n",
      "     |      \n",
      "     |      Simple example that uses batches:\n",
      "     |      TODO: Add simplified batch example here\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      ----------\n",
      "     |      tensorflow_mcmc.sampling.mcmc_base_classes.BurnInMCMCSampler:\n",
      "     |          Base class for `SGLDSampler` that specifies how actual sampling\n",
      "     |          is performed (using iterator protocol, e.g. `next(sampler)`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      "     |  \n",
      "     |  __next__(self)\n",
      "     |      Perform a sampler step:\n",
      "     |          Compute and return the next sample and next cost values\n",
      "     |          for this sampler.\n",
      "     |      \n",
      "     |          While `self.is_burning_in` returns `True`\n",
      "     |          (while the sampler has not yet performed `self.burn_in_steps`\n",
      "     |          steps) this will also adapt the samplers mass matrix in a\n",
      "     |          sampler-specific way to improve performance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sample: list of numpy.ndarray objects\n",
      "     |          Sampled values are a `numpy.ndarray` for each target parameter.\n",
      "     |      \n",
      "     |      cost: numpy.ndarray (1,)\n",
      "     |          Current cost value of the last evaluated target parameter values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      "     |  \n",
      "     |  is_burning_in\n",
      "     |      Check if this sampler is still in burn-in phase.\n",
      "     |          Used during graph construction to insert conditionals into the\n",
      "     |          graph that will make the sampler skip all burn-in operations\n",
      "     |          after the burn-in phase is over.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_burning_in: boolean\n",
      "     |          `True` if `self.n_iterations <= self.burn_in_steps`, otherwise `False`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      "     |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      "     |      \n",
      "     |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      "     |      directly, and then acts as a mix-in class.  You can also register\n",
      "     |      unrelated concrete classes (even built-in classes) and unrelated\n",
      "     |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      "     |      be considered subclasses of the registering ABC by the built-in\n",
      "     |      issubclass() function, but the registering ABC won't show up in\n",
      "     |      their MRO (Method Resolution Order) nor will method\n",
      "     |      implementations defined by the registering ABC be callable (not\n",
      "     |      even via super()).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pysgmcmc.sampling.MCMCSampler:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Allows using samplers as iterators.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      ----------\n",
      "     |      Extract the first three thousand samples (with costs) from a sampler:\n",
      "     |      \n",
      "     |      >>> import tensorflow as tf\n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from itertools import islice\n",
      "     |      >>> from pysgmcmc.samplers.sghmc import SGHMCSampler\n",
      "     |      >>> session = tf.Session()\n",
      "     |      >>> x = tf.Variable(1.0)\n",
      "     |      >>> dist = tf.contrib.distributions.Normal(loc=0., scale=1.)\n",
      "     |      >>> n_burn_in, n_samples = 1000, 2000\n",
      "     |      >>> sampler = SGHMCSampler(params=[x], burn_in_steps=n_burn_in, cost_fun=lambda x: -dist.log_prob(x), session=session, dtype=tf.float32)\n",
      "     |      >>> session.run(tf.global_variables_initializer())\n",
      "     |      >>> burn_in_samples = list(islice(sampler, n_burn_in))  # perform all burn_in steps\n",
      "     |      >>> samples = list(islice(sampler, n_samples))\n",
      "     |      >>> len(burn_in_samples), len(samples)\n",
      "     |      (1000, 2000)\n",
      "     |      >>> session.close()\n",
      "     |      >>> tf.reset_default_graph()  # to avoid polluting test environment\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pysgmcmc.sampling.MCMCSampler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['SGHMCSampler', 'SGLDSampler']\n",
      "\n",
      "FILE\n",
      "    /home/moritz/pysgmcmc/pysgmcmc/samplers/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pg.samplers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler hyperparameters\n",
    "\n",
    "To get a clearer picture of all possible design choices when instantiating any of \n",
    "our samplers, consider our docstrings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGHMCSampler in module pysgmcmc.samplers.sghmc:\n",
      "\n",
      "class SGHMCSampler(pysgmcmc.sampling.BurnInMCMCSampler)\n",
      " |  Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses a burn-in\n",
      " |  procedure to adapt its own hyperparameters during the initial stages\n",
      " |  of sampling.\n",
      " |  \n",
      " |  See [1] for more details on this burn-in procedure.\n",
      " |  See [2] for more details on Stochastic Gradient Hamiltonian Monte-Carlo.\n",
      " |  \n",
      " |  [1] J. T. Springenberg, A. Klein, S. Falkner, F. Hutter\n",
      " |      Bayesian Optimization with Robust Bayesian Neural Networks.\n",
      " |      In Advances in Neural Information Processing Systems 29 (2016).\n",
      " |  \n",
      " |  [2] T. Chen, E. B. Fox, C. Guestrin\n",
      " |      Stochastic Gradient Hamiltonian Monte Carlo\n",
      " |      In Proceedings of Machine Learning Research 32 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGHMCSampler\n",
      " |      pysgmcmc.sampling.BurnInMCMCSampler\n",
      " |      pysgmcmc.sampling.MCMCSampler\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, cost_fun, seed=None, batch_generator=None, epsilon=0.01, session=None, burn_in_steps=3000, scale_grad=1.0, dtype=tf.float64, mdecay=0.05)\n",
      " |      Initialize the sampler parameters and set up a tensorflow.Graph\n",
      " |          for later queries.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : list of tensorflow.Variable objects\n",
      " |          Target parameters for which we want to sample new values.\n",
      " |      \n",
      " |      cost_fun : callable\n",
      " |          Function that takes `params` as input and returns a\n",
      " |          1-d `tensorflow.Tensor` that contains the cost-value.\n",
      " |          Frequently denoted with `U` in literature.\n",
      " |      \n",
      " |      seed : int, optional\n",
      " |          Random seed to use.\n",
      " |          Defaults to `None`.\n",
      " |      \n",
      " |      batch_generator : iterable, optional\n",
      " |          Iterable which returns dictionaries to feed into\n",
      " |          tensorflow.Session.run() calls to evaluate the cost function.\n",
      " |          Defaults to `None` which indicates that no batches shall be fed.\n",
      " |      \n",
      " |      epsilon : float, optional\n",
      " |          Value that is used as learning rate parameter for the sampler,\n",
      " |          also denoted as discretization parameter in literature.\n",
      " |          Defaults to `0.01`.\n",
      " |      \n",
      " |      session : tensorflow.Session, optional\n",
      " |          Session object which knows about the external part of the graph\n",
      " |          (which defines `Cost`, and possibly batches).\n",
      " |          Used internally to evaluate (burn-in/sample) the sampler.\n",
      " |      \n",
      " |      burn_in_steps: int, optional\n",
      " |          Number of burn-in steps to perform. In each burn-in step, this\n",
      " |          sampler will adapt its own internal parameters to decrease its error.\n",
      " |          For reference see: TODO ADD PAPER REFERENCE HERE\n",
      " |      \n",
      " |      scale_grad : float, optional\n",
      " |          Value that is used to scale the magnitude of the noise used\n",
      " |          during sampling. In a typical batches-of-data setting this usually\n",
      " |          corresponds to the number of examples in the entire dataset.\n",
      " |          Defaults to `1.0` which corresponds to no scaling.\n",
      " |      \n",
      " |      dtype : tensorflow.DType, optional\n",
      " |          Type of elements of `tensorflow.Tensor` objects used in this sampler.\n",
      " |          Defaults to `tensorflow.float64`.\n",
      " |      \n",
      " |      mdecay : float, optional\n",
      " |          (Constant) momentum decay per time-step.\n",
      " |          Defaults to `0.05`.\n",
      " |      \n",
      " |      Examples\n",
      " |      ----------\n",
      " |      Simple, plain example:\n",
      " |      TODO: Add 2D Gaussian Case here\n",
      " |      \n",
      " |      Simple example that uses batches:\n",
      " |      TODO: Add simplified batch example here\n",
      " |      \n",
      " |      See Also\n",
      " |      ----------\n",
      " |      pysgmcmc.sampling.BurnInMCMCSampler:\n",
      " |          Base class for `SGHMCSampler` that specifies how actual sampling\n",
      " |          is performed (using iterator protocol, e.g. `next(sampler)`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      " |  \n",
      " |  __next__(self)\n",
      " |      Perform a sampler step:\n",
      " |          Compute and return the next sample and next cost values\n",
      " |          for this sampler.\n",
      " |      \n",
      " |          While `self.is_burning_in` returns `True`\n",
      " |          (while the sampler has not yet performed `self.burn_in_steps`\n",
      " |          steps) this will also adapt the samplers mass matrix in a\n",
      " |          sampler-specific way to improve performance.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sample: list of numpy.ndarray objects\n",
      " |          Sampled values are a `numpy.ndarray` for each target parameter.\n",
      " |      \n",
      " |      cost: numpy.ndarray (1,)\n",
      " |          Current cost value of the last evaluated target parameter values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      " |  \n",
      " |  is_burning_in\n",
      " |      Check if this sampler is still in burn-in phase.\n",
      " |          Used during graph construction to insert conditionals into the\n",
      " |          graph that will make the sampler skip all burn-in operations\n",
      " |          after the burn-in phase is over.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      is_burning_in: boolean\n",
      " |          `True` if `self.n_iterations <= self.burn_in_steps`, otherwise `False`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pysgmcmc.sampling.BurnInMCMCSampler:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pysgmcmc.sampling.MCMCSampler:\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Allows using samplers as iterators.\n",
      " |      \n",
      " |      Examples\n",
      " |      ----------\n",
      " |      Extract the first three thousand samples (with costs) from a sampler:\n",
      " |      \n",
      " |      >>> import tensorflow as tf\n",
      " |      >>> import numpy as np\n",
      " |      >>> from itertools import islice\n",
      " |      >>> from pysgmcmc.samplers.sghmc import SGHMCSampler\n",
      " |      >>> session = tf.Session()\n",
      " |      >>> x = tf.Variable(1.0)\n",
      " |      >>> dist = tf.contrib.distributions.Normal(loc=0., scale=1.)\n",
      " |      >>> n_burn_in, n_samples = 1000, 2000\n",
      " |      >>> sampler = SGHMCSampler(params=[x], burn_in_steps=n_burn_in, cost_fun=lambda x: -dist.log_prob(x), session=session, dtype=tf.float32)\n",
      " |      >>> session.run(tf.global_variables_initializer())\n",
      " |      >>> burn_in_samples = list(islice(sampler, n_burn_in))  # perform all burn_in steps\n",
      " |      >>> samples = list(islice(sampler, n_samples))\n",
      " |      >>> len(burn_in_samples), len(samples)\n",
      " |      (1000, 2000)\n",
      " |      >>> session.close()\n",
      " |      >>> tf.reset_default_graph()  # to avoid polluting test environment\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pysgmcmc.sampling.MCMCSampler:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pg.samplers.SGHMCSampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting samples\n",
    "\n",
    "Extracting the next sample (with corresponding costs) from any of our samplers always simply amounts to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8457049a35d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sampler' is not defined"
     ]
    }
   ],
   "source": [
    "sample, cost = next(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "This interface allows us to extract samples in different contexts:\n",
    "\n",
    "1. extract a chain of n subsequent samples\n",
    "2. sample until an external event occurs / an external condition becomes `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-708c3a272f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampler' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. extract a chain of n subsequent samples\n",
    "samples, n = [], 1000\n",
    "\n",
    "\n",
    "for _ in range(n):\n",
    "    sample, _ = next(sampler)\n",
    "    samples.append(sample)\n",
    "\n",
    "# shorthand for 1., using itertools.islice\n",
    "import itertools\n",
    "samples = [sample for sample, _ in itertools.islice(sampler, n)]\n",
    "    \n",
    "# 2. sample until an external event occurs\n",
    "\n",
    "# dummy event\n",
    "def external_event():\n",
    "    return np.random.randint(0, 10) > 5\n",
    "\n",
    "samples = []\n",
    "while not external_event():\n",
    "    sample, _ = next(sampler)\n",
    "    samples.append(sample)\n",
    "    \n",
    "    \n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interface also allows us to use any of our samplers in (infinite) for-loops. \n",
    "\n",
    "But *be warned*: such a for-loop will **not terminate** unless you explicitly break out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-028de5879acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampler' is not defined"
     ]
    }
   ],
   "source": [
    "samples, i = [], 0\n",
    "for sample, cost in sampler:\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing chains/traces of samples\n",
    "\n",
    "To analyze the results of a sampler run, we transform the results obtained by our samplers into `pymc3.MultiTrace` objects. Then we can use the (well-established) `pymc3` machinery to compute diagnostics for our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pysgmcmc.diagnostics.sample_chains import PYSGMCMCTrace\n",
    "\n",
    "# XXX: Compute PYSGMCMCTrace (and possibly pymc3.MultiTrace from those) and \n",
    "# use those to compute e.g. ess and maybe produce some plots too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we also provide a shortcut function that directly computes a multitrace for one of our samplers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pymc3_multitrace in module pysgmcmc.diagnostics.sample_chains:\n",
      "\n",
      "pymc3_multitrace(get_sampler, n_chains=2, samples_per_chain=100, parameter_names=None)\n",
      "    Extract chains from `sampler` and return them as `pymc3.MultiTrace`\n",
      "        object.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    sampler : pysgmcmc.sampling.MCMCSampler subclass\n",
      "        An instance of one of our samplers.\n",
      "    \n",
      "    parameter_names : List[String] or NoneType, optional\n",
      "        List of names for each target parameter of the sampler.\n",
      "        If set to `None`, simply enumerate the parameters and use those numbers\n",
      "        as names.\n",
      "        Defaults to `None`.\n",
      "    \n",
      "    Returns\n",
      "    ----------\n",
      "    multitrace : pymc3.backends.base.MultiTrace\n",
      "        TODO: DOKU\n",
      "    \n",
      "    \n",
      "    Examples\n",
      "    ----------\n",
      "    TODO ADD EXAMPLE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pg.diagnostics.sample_chains.pymc3_multitrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PYSGMCMC - trained BNN\n",
    "\n",
    "We provide an implementation of a Bayesian Neural Network that is trained using our samplers. \n",
    "\n",
    "The (tensorflow-) architecture of this BNN can be customized by the user and any of our sampling methods can be used to sample networks during training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XXX: Showcase our BNN on some hpolib function here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
